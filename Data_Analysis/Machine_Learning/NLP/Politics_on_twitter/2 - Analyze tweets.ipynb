{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "388aeddf",
   "metadata": {},
   "source": [
    "# Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f64fc90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import tweepy\n",
    "import webbrowser\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "import unidecode\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6437b7c1",
   "metadata": {},
   "source": [
    "# Import keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44d46f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from raw_data import keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a1d9f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_api_key = keys.twitter_api_key()\n",
    "twitter_api_secret_key = keys.twitter_api_key_secret()\n",
    "twitter_bearer_token = keys.twitter_bearer_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bdc037",
   "metadata": {},
   "source": [
    "# Connect my developer project to my twitter profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f5f751c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.twitter.com/oauth/authorize?oauth_token=3_tTdwAAAAABUe3MAAABfHWfA_c\n"
     ]
    }
   ],
   "source": [
    "callback_uri = \"oob\" # url\n",
    "auth = tweepy.OAuthHandler(twitter_api_key, twitter_api_secret_key, callback_uri)\n",
    "redirect_url = auth.get_authorization_url()\n",
    "print(redirect_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73561702",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1928120398.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_31346/1928120398.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    m=\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "m="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae9a2dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pin_value = '0307105'\n",
    "auth.get_access_token(user_pin_value)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd893808",
   "metadata": {},
   "source": [
    "# Load new content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93f5fb7",
   "metadata": {},
   "source": [
    "## Function to extract the tweets of politicals that I follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43e2edea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tweets_from_politics(number_of_tweet_per_politic):\n",
    "    \"\"\"This function return a DataFrame of the last tweets of politics that I follow on Twitter\"\"\"\n",
    "    \n",
    "    # Start by working on my own account\n",
    "    me = api.get_user(screen_name=\"alecoursonnois\")\n",
    "    # Generate a list of my all of my friends (considering that I have under 200 friends on twitter)\n",
    "    my_friends = me.friends(count=200)\n",
    "    \n",
    "    # Retrieve my personal friends which I don't want to analyze their tweets\n",
    "    not_politics = [\"alecoursonnois\", \"Baly_5\", \"LucieCMP\", \"JeannonoSmith\", \"Seezzy_\"]\n",
    "    \n",
    "    # Create a set to stock the DataFrame columns names\n",
    "    columns = set()\n",
    "    # Create a list to restrict the extractions to string and int datas only\n",
    "    allowed_types = [str, int]\n",
    "    # Create a list to save each tweet and its datas\n",
    "    tweets_data = []\n",
    "    # Create to save politics screen_names\n",
    "    politics_screen_name = []\n",
    "    \n",
    "    # Iterate on my_friends\n",
    "    for friend in my_friends:\n",
    "        friend_name = friend.screen_name\n",
    "        \n",
    "        # Don't act if this friend is out of the project\n",
    "        if friend_name in not_politics:\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            # Generate politics timeline list of the last tweets (the number must be indicated as a function's argument)\n",
    "            timeline_list = api.get_user(screen_name=friend_name).timeline(count=number_of_tweet_per_politic)\n",
    "            \n",
    "            # Iterate on each tweet\n",
    "            for status in timeline_list:\n",
    "                # Generate a dictionary of status attributes\n",
    "                status_dict = dict(vars(status))\n",
    "                \n",
    "                # From this dictionary, get the keys and stock it in a variable\n",
    "                keys = vars(status).keys()\n",
    "                \n",
    "                # Create a dictionary stocking each status of the politic we are working on\n",
    "                # Integrate the status user_screen_name and its author_screen_name\n",
    "                # This dictionary will be added to the \"tweets_data\" list and finaly convert into a DataFrame\n",
    "                single_tweet_data = {\"user\":status.user.screen_name, \"author\": status.author.screen_name}\n",
    "               \n",
    "                # Iterate on each status key (future column name)\n",
    "                for k in keys:\n",
    "                    # Check its type\n",
    "                    v_type = type(status_dict[k])\n",
    "                    \n",
    "                    # If its in the allowed_types list there we go\n",
    "                    if v_type in allowed_types:\n",
    "                        # Add the status data to its right key into the dictionary of each status\n",
    "                        single_tweet_data[k] = status_dict[k]\n",
    "                        # Add the key as a column name in the set\n",
    "                        columns.add(k)\n",
    "                \n",
    "                # Append the full single_tweet_data dictionary with the right key to the tweets_data list\n",
    "                tweets_data.append(single_tweet_data)\n",
    "\n",
    "    # Give a name to the columns respecting the order\n",
    "    headers_cols = list(columns)\n",
    "    # Add user and author column names\n",
    "    headers_cols.append(\"user\")\n",
    "    headers_cols.append(\"author\")\n",
    "\n",
    "    # Convert the tweets_data list into a DataFrame\n",
    "    df = pd.DataFrame(tweets_data)\n",
    "    \n",
    "    # Only keep columns that provides information\n",
    "    columns_to_keep = ['user', 'author', 'id', 'text', 'source', 'source_url']\n",
    "    df = df[columns_to_keep]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e489b9b",
   "metadata": {},
   "source": [
    "## Apply the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6d76d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_tweets_df = extract_tweets_from_politics(number_of_tweet_per_politic=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738807e0",
   "metadata": {},
   "source": [
    "## Add features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f187124",
   "metadata": {},
   "source": [
    "- Parti politique \n",
    "- Tendance politique\n",
    "- Date d'extraction (pour intégrer cette information il faudrait créer une copie de l'archive sans cette colonne pour pouvoir retirer les doublons, puis, merger le df sans doublon au sauvegardé pour avoir la date d'extraction)\n",
    "- Score par lexique (plus tard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55167ffb",
   "metadata": {},
   "source": [
    "### Generate new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d59e36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = [{\"user\":'EPhilippe_LH', \"political_party\":\"horizons\", \"political_trend\":\"right\"},\n",
    "                 {\"user\":'ZemmourEric', \"political_party\":\"zemmour\", \"political_trend\":\"extrem_right\"},\n",
    "                 {\"user\":'EmmanuelMacron', \"political_party\":\"LREM\", \"political_trend\":\"right\"},\n",
    "                 {\"user\":'Waechter2022', \"political_party\":\"mouvement_ecologiste_independant\", \"political_trend\":\"ecology\"},\n",
    "                 {\"user\":'Fabien_Roussel', \"political_party\":\"PCF\", \"political_trend\":\"extrem_left\"},\n",
    "                 {\"user\":'PhilippePoutou', \"political_party\":\"nouveau_parti_anticapitaliste\", \"political_trend\":\"extrem_left\"},\n",
    "                 {\"user\":'jfpoisson78', \"political_party\":\"la_voie_du_peuple\", \"political_trend\":\"extrem_right\"},\n",
    "                 {\"user\":'f_philippot', \"political_party\":\"les_patriotes\", \"political_trend\":\"extrem_right\"},\n",
    "                 {\"user\":'vpecresse', \"political_party\":\"soyons_libres\", \"political_trend\":\"right\"},\n",
    "                 {\"user\":'DenisPayre', \"political_party\":\"les_republicains\", \"political_trend\":\"right\"},\n",
    "                 {\"user\":'montebourg', \"political_party\":\"l_engagement\", \"political_trend\":\"left\"},\n",
    "                 {\"user\":'JLMelenchon', \"political_party\":\"la_france_insoumise\", \"political_trend\":\"extrem_left\"},\n",
    "                 {\"user\":'MLP_officiel', \"political_party\":\"front_national\", \"political_trend\":\"extrem_right\"},\n",
    "                 {\"user\":'SLeFoll', \"political_party\":\"parti_socialiste\", \"political_trend\":\"left\"},\n",
    "                 {\"user\":'jeanlassalle', \"political_party\":\"resistons\", \"political_trend\":\"divers\"},\n",
    "                 {\"user\":'larrouturou', \"political_party\":\"nouvelle_donne\", \"political_trend\":\"left\"},\n",
    "                 {\"user\":'philippejuvin', \"political_party\":\"les_republicains\", \"political_trend\":\"right\"},\n",
    "                 {\"user\":'yjadot', \"political_party\":\"EELV\", \"political_trend\":\"ecology\"},\n",
    "                 {\"user\":'Anne_Hidalgo', \"political_party\":\"parti_socialiste\", \"political_trend\":\"left\"},\n",
    "                 {\"user\":'gerardfiloche', \"political_party\":\"la_gauche_democratique_et_sociale\", \"political_trend\":\"extrem_left\"},\n",
    "                 {\"user\":'dupontaignan', \"political_party\":\"debout_la_france\", \"political_trend\":\"extrem_right\"},\n",
    "                 {\"user\":'ECiotti', \"political_party\":\"les_republicains\", \"political_trend\":\"right\"},\n",
    "                 {\"user\":'xavierbertrand', \"political_party\":\"la_manufacture\", \"political_trend\":\"right\"},\n",
    "                 {\"user\":'MichelBarnier', \"political_party\":\"les_republicains\", \"political_trend\":\"right\"},\n",
    "                 {\"user\":'UPR_Asselineau', \"political_party\":\"union_populaire_republicaine\", \"political_trend\":\"divers\"},\n",
    "                 {\"user\":'n_arthaud', \"political_party\":\"lutte_ouvriere\", \"political_trend\":\"extrem_left\"},\n",
    "                 {\"user\":'MartinRocca12', \"political_party\":\"constituante_2022\", \"political_trend\":\"divers\"},\n",
    "                 {\"user\":'gilleslazzarini', \"political_party\":\"parti_politique_pour_la_paix_et_la_protection_de_la_planete\", \"political_trend\":\"divers\"},\n",
    "                 {\"user\":'AlexLanglois_', \"political_party\":\"refondation_2022\", \"political_trend\":\"divers\"},\n",
    "                 {\"user\":'luclaf', \"political_party\":\"une_perspective_la_6e_republique\", \"political_trend\":\"divers\"},\n",
    "                 {\"user\":'Vukuzman', \"political_party\":\"republique_souveraine\", \"political_trend\":\"divers\"},\n",
    "                 {\"user\":'FabriceGrimal', \"political_party\":\"la_concorde_citoyenne_2022\", \"political_trend\":\"divers\"},\n",
    "                 {\"user\":'ClaraEgger1', \"political_party\":\"espoir_RIC_2022\", \"political_trend\":\"divers\"},\n",
    "                 {\"user\":'Cau_Marie_', \"political_party\":\"sans_etiquette\", \"political_trend\":\"divers\"},\n",
    "                 {\"user\":'antoine27955080', \"political_party\":\"volontaires_pour_la_france\", \"political_trend\":\"extrem_right\"},\n",
    "                 {\"user\":'regis_ollivier', \"political_party\":\"independant\", \"political_trend\":\"right\"},\n",
    "                 {\"user\":'JaclineMouraud', \"political_party\":\"sans_etiquette\", \"political_trend\":\"right\"},\n",
    "                 {\"user\":'HeleneThouy', \"political_party\":\"parti_animaliste\", \"political_trend\":\"ecology\"},\n",
    "                 {\"user\":'MAZUEL_Pace', \"political_party\":\"pace\", \"political_trend\":\"left\"},\n",
    "                 {\"user\":'MarCharlott', \"political_party\":\"sans_etiquette\", \"political_trend\":\"left\"},\n",
    "                 {\"user\":'AguebPorterie', \"political_party\":\"aucun\", \"political_trend\":\"left\"},\n",
    "                 {\"user\":'AnasseKazib', \"political_party\":\"courant_communiste_revolutionnaire_revolution_permanente\", \"political_trend\":\"extrem_left\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "763ebb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features_df = pd.DataFrame(new_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089146ae",
   "metadata": {},
   "source": [
    "### Merge it to last_tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dc7c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_tweets_df = pd.merge(left=last_tweets_df, right=new_features_df, on='user', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e21a21",
   "metadata": {},
   "source": [
    "## Check the number of tweets per politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bb903b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_tweets_df[\"user\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1084186c",
   "metadata": {},
   "source": [
    "## Add it to the main global DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e83f37",
   "metadata": {},
   "source": [
    "### Load the global_df \n",
    "- and drop the last index column automatically named \"Unamed: 0\" when saving df to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e548f5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_df = pd.read_csv(\"global_df.csv\").drop(columns=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658bf739",
   "metadata": {},
   "source": [
    "### Add new content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bc89a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We just added 12 tweets to our global_df\n"
     ]
    }
   ],
   "source": [
    "actual_number_of_tweets = global_df.shape[0]\n",
    "global_df = global_df.append(last_tweets_df, ignore_index=True)\n",
    "global_df = global_df.drop_duplicates()\n",
    "print(f\"We just added {global_df.shape[0]-actual_number_of_tweets} tweets to our global_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332b34ba",
   "metadata": {},
   "source": [
    "### Save the new global_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b520eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8735, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67efbe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_df.to_csv(\"global_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe632af",
   "metadata": {},
   "source": [
    "# Clean tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28ec9a9",
   "metadata": {},
   "source": [
    "## Make a new df from the global one\n",
    "- This way we will be allowed to drop duplicates next time we will load the global_df from the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b0fce02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready_for_nlp = global_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeed460",
   "metadata": {},
   "source": [
    "## Retrieve recurrent regex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d320cc2d",
   "metadata": {},
   "source": [
    "### Retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bb457d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_rt_retweets(df):\n",
    "    new_texts = []\n",
    "    \n",
    "    for text in df[\"text\"]:\n",
    "        if text[:3] == 'RT ' or text[:3] == 'rt ':\n",
    "            new_texts.append(text[3:])\n",
    "        else:\n",
    "            new_texts.append(text)\n",
    "    \n",
    "    df[\"text\"] = new_texts\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc232a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready_for_nlp = retrieve_rt_retweets(df_ready_for_nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090cc107",
   "metadata": {},
   "source": [
    "### Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afb64d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_links(df):\n",
    "    pattern = re.compile(\"https://*\")\n",
    "    new_text = []\n",
    "\n",
    "    for text in df[\"text\"]:\n",
    "        if re.search(pattern, text) is None:\n",
    "            new_text.append(text)\n",
    "        else:\n",
    "            match_position = re.search(pattern, text).span()[0]\n",
    "            new_text.append(text[:(match_position-1)])\n",
    "\n",
    "    df[\"text\"] = new_text\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c093ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready_for_nlp = retrieve_links(df_ready_for_nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29273dd9",
   "metadata": {},
   "source": [
    "## Define a text_cleaner function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55cff025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    # Remove Punctuation\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, ' ') \n",
    "    \n",
    "    # Lower Case\n",
    "    lowercased = text.lower() \n",
    "    \n",
    "    # Remove accents\n",
    "    unaccented_string = unidecode.unidecode(lowercased) \n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized = word_tokenize(unaccented_string) \n",
    "    \n",
    "    # Remove numbers\n",
    "    words_only = [word for word in tokenized if word.isalpha()] \n",
    "    \n",
    "    # Make stopword list\n",
    "    stop_words = set(stopwords.words('french')) \n",
    "    \n",
    "    # Remove Stop Words\n",
    "    without_stopwords = [word for word in words_only if not word in stop_words]\n",
    "    \n",
    "    return \" \".join(without_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d50d6a",
   "metadata": {},
   "source": [
    "## Apply the text_cleaner function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2d4bb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready_for_nlp['text'] = df_ready_for_nlp['text'].apply(text_cleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2077576",
   "metadata": {},
   "source": [
    "# Analyze tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0680ab41",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Vectorize the text and estimate the associated weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "342c1544",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Tuned TFidfvectorizer\n",
    "# vec = TfidfVectorizer(ngram_range = (1,1), min_df=0.01, max_df = 0.05).fit(df_ready_for_nlp[\"text\"])\n",
    "\n",
    "# # Transform text to vectors\n",
    "# vectors = vec.transform(df_ready_for_nlp[\"text\"]) \n",
    "\n",
    "# # Sum of tfidf weighting by word\n",
    "# sum_tfidf = vectors.sum(axis=0) \n",
    "\n",
    "# # Get the word and associated weight\n",
    "# tfidf_list = [(word, sum_tfidf[0, idx]) for word, idx in vec.vocabulary_.items()]  \n",
    "\n",
    "# # Sort\n",
    "# sorted_tfidf_list =sorted(tfidf_list, key = lambda x: x[1], reverse=True)  \n",
    "\n",
    "# sorted_tfidf_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876fd074",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Map words weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7858cc0a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- To retrieve once again stopwords but with WordCloud, add after \"height\":\n",
    "- stopwords=STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6b36d163",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.subplots(figsize=(25,15))\n",
    "# wordcloud = WordCloud(background_color='white', width=1920, height=1080, stopwords=STOPWORDS).generate(\" \".join(df_ready_for_nlp[\"text\"]))\n",
    "# plt.imshow(wordcloud)\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c377183",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Count iteration per word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0b44aaf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# words_iteration = {}\n",
    "\n",
    "# for text in df_ready_for_nlp[\"text\"]:\n",
    "#     tokenized_word = word_tokenize(text)\n",
    "#     for word in tokenized_word:\n",
    "#         if word in words_iteration:\n",
    "#             words_iteration[word] += 1\n",
    "#         else:\n",
    "#             words_iteration[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d83d81cb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# len(words_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8a9bfcce",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# dict(sorted(words_iteration.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888f4cc9",
   "metadata": {},
   "source": [
    "## Work on lexical contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81806c1",
   "metadata": {},
   "source": [
    "### Define lexical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27e6bbd",
   "metadata": {},
   "source": [
    "- Politique V\n",
    "- Ecologie\n",
    "- Socialiste\n",
    "- Capitaliste\n",
    "- Communiste\n",
    "- Extreme\n",
    "- Humaniste\n",
    "- Santé\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1075831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "political_lexicon = \"\"\"démocratie,politicien,gouvernement,état,diplomatie,fédéralisme,constitution,socialisme,idéologie,économique,économie,libéralisme,régime,pouvoir,social,actualité,citoyen,monarchie,parti,Machiavel,pluralisme,opposition,parlement,stratégie,civique,dictature,gouvernant,machiavélisme,parti politique,réactionnaire,dépolitiser,diplomatique,fascisme,géopolitique,politiser,sionisme,homme d'État,opinion,politologue,progressiste,révolution,révolutionnaire,technocratie,apartheid,écologie,égalité,gauche,indépendance,nationalisme,oligarchie,populisme,réfugié,terrorisme,austérité,révolution,démagogie,polémique,diplomate,philosophe,politisation,grève,libéral,programme,sociologie,souveraineté,système,alliance,anarchie,autocratie,bipolarisation,congrès,opportunisme,politiquement,propagande,schisme,sénat,Union européenne,apolitique,débat,féodalité,impérialisme,leader,militant,parlementaire,politicard,scission,économiste,extrémisme,impérialiste,analyste,centriste,écologiste,immigration,machiavélique,arène,bourgeoisie,communiste,instabilité,mondialisation,pamphlet,science politique,société,culturel,dissidence,européenne,financier,journal,mission,multipartisme,panafricanisme,racisme,anarchisme,civil,crise,électeur,gaullisme,goulag,islamisme,majorité,marxiste,orientation,parti républicain,protectorat,scandale,terreur,transition,utopie,amnistie,coalition,courant,décentralisation,entente,extrémiste,gouvernementale,Mao ZeDong,municipale,philosophie,philosophique,relance,revue,statut,technocrate,bonapartiste,clientélisme,démocratique,religieuse,répression,séparatiste,souverainiste,capitalisme,conduite,conflit,idéologique,isolationnisme,juridique,morales,politologie,publiciste,autarcie,bureaucratie,cléricalisme,colonial,contexte,démocrate,dialogue,gestion,glasnost,institutionnel,politicailler,politicaillerie,stabilité,totalitaire,vie politique,agitation,anticléricalisme,appartenance,autruche,barre,bureau,centre,club,écologisme,éditorial,état-providence,gauchiste,gazette,homme,infléchir,institut,institutions,intégration,keynésianisme,modéré,monarchiste,morale,violence,orthodoxe,panslavisme,pression,prolétariat,province,sécession,tendance,totalitarisme,action,activiste,affaire,constitutionnelle,déflation,échiquier,encarté,extérieure,faction,habile,homme politique,indépendantiste,intérieure,interpellation,interventionnisme,junte,Marine Le Pen,nation,parlementarisme,public,quitter,réaction,religion,social-démocratie,souverainisme,sphère,syndicalisme,tribalisme,troïka,agrarien,ambiant,budgétaire,considérations,convictions,discrimination,droits civils,fédérale,féminisme,finance,Guépéou,intrigues,langue de bois,monétaire,nationalité,nihilisme,réforme,Tchéka,tempête,administrative,ambitions,assemblées,autonomie,classe ouvrière,commerciales,conservateur,conversion,ethnique,fédération,financement,intérêts,littérature,menées,nationale,opposant,personnalités,populiste,protectionniste,querelles,républicain,syndical,affaires publiques,artistiques,démographique,discrimination positive,gouverner,habileté,indépendantisme,négociation,plan,police,politiste,ruse,science,temporisation,Thucydide,abstentionniste,Académie des sciences,accession,activisme,agora,aliénation,ambitieuse,anarchiste,antiparlementaire,autonome,autonomiste\"\"\"\n",
    "political_lexicon = word_tokenize(text_cleaner(political_lexicon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce3300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecology_lexicon = \"\"\"\"\"\"\n",
    "ecology_lexicon = word_tokenize(text_cleaner(ecology_lexicon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3816696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "socialist_lexicon = \"\"\"\"\"\"\n",
    "socialist_lexicon = word_tokenize(text_cleaner(socialist_lexicon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dbea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "capitalist_lexicon = \"\"\"\"\"\"\n",
    "capitalist_lexicon = word_tokenize(text_cleaner(capitalist_lexicon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2cd538",
   "metadata": {},
   "outputs": [],
   "source": [
    "communist_lexicon = \"\"\"\"\"\"\n",
    "communist_lexicon = word_tokenize(text_cleaner(communist_lexicon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9987a8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "extrem_lexicon = \"\"\"\"\"\"\n",
    "extrem_lexicon = word_tokenize(text_cleaner(extrem_lexicon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae438a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "humanist_lexicon = \"\"\"\"\"\"\n",
    "humanist_lexicon = word_tokenize(text_cleaner(humanist_lexicon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a7386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "health_lexicon = \"\"\"\"\"\"\n",
    "health_lexicon = word_tokenize(text_cleaner(health_lexicon))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6258d414",
   "metadata": {},
   "source": [
    "### Define a function to create a df with a lexicon score per user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea06faa",
   "metadata": {},
   "source": [
    "#### Store all the lexicons in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a15452a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f650bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "104b83cf",
   "metadata": {},
   "source": [
    "#### Define the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8695c46b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cee93c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ba9b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfa5db81",
   "metadata": {},
   "source": [
    "#### Apply it to df_ready_for_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a8ffc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc43e609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27d5ee28",
   "metadata": {},
   "source": [
    "### Define a function to count each time a twitter user user a word present in a given lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68834c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexicon_importance_per_user_df(df, lexicon):\n",
    "    \"\"\"Return a DataFrame couting each time a twitter user use a word present in a given lexicon\"\"\"\n",
    "    # Define the users\n",
    "    list_of_users = list(df[\"user\"].unique())\n",
    "    # Define a list stocking the results per user\n",
    "    lexicon_per_user_list = []\n",
    "\n",
    "    for user in list_of_users:\n",
    "        # Define a DataFrame per user\n",
    "        users_personnal_df = df[df[\"user\"]==user]\n",
    "        # Analyze each tweet of each personnal DataFrame created\n",
    "        for text in users_personnal_df[\"text\"]:\n",
    "            # Tokenize tweets\n",
    "            tokenized_word = word_tokenize(text)\n",
    "            # Iterate over each one of them\n",
    "            for word in tokenized_word:\n",
    "                # Check if they are in a certain lexicon\n",
    "                if word in lexicon:\n",
    "                    # If it is the case, create a dictionary that stock this information\n",
    "                    new_row = {\"user\":user}\n",
    "                    # If the word is already registered in this new_row count +1\n",
    "                    if word in new_row:\n",
    "                        new_row[word] += 1\n",
    "                    # If it is the first time, only count 1\n",
    "                    else:\n",
    "                        new_row[word] = 1\n",
    "                    # Finaly, append this new_row to the lexic_per_user_list\n",
    "                    lexicon_per_user_list.append(new_row)\n",
    "                else:\n",
    "                    pass\n",
    "    \n",
    "    # Generate the DataFrame from the lexicon_per_user_list just created\n",
    "    lexicon_per_user_df = pd.DataFrame(lexicon_per_user_list)\n",
    "    # Fillna by 0\n",
    "    lexicon_per_user_df = lexicon_per_user_df.fillna(0)\n",
    "    # Groupby and sum the results per user\n",
    "    lexicon_per_user_df = lexicon_per_user_df.groupby(['user']).sum()\n",
    "    # Create a \"total\" column to generate a lexicon_score_per_user\n",
    "    lexicon_per_user_df[\"lexicon_score_per_user\"] = lexicon_per_user_df.sum(axis=1)\n",
    "    # Transpose the DataFrame to have the users in column and the words in row\n",
    "    lexicon_per_user_df_transposed = lexicon_per_user_df.T\n",
    "    \n",
    "    return lexicon_per_user_df_transposed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3551170e",
   "metadata": {},
   "source": [
    "### Apply the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5d5fcec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test = lexicon_importance_per_user_df(df=df_ready_for_nlp, lexicon=political_lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf719d3c",
   "metadata": {},
   "source": [
    "### Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cb3a0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_score = df_test.loc[\"lexicon_score_per_user\"]\n",
    "user_top_words = df_test[\"Anne_Hidalgo\"].sort_values(ascending=False)[1:]\n",
    "\n",
    "for word, recurrence_results in user_top_words.items():\n",
    "    if recurrence_results == 0:\n",
    "        user_top_words = user_top_words.drop(labels=word)\n",
    "        \n",
    "top_20_user_top_words = user_top_words[:20]\n",
    "\n",
    "plot_index = list(top_20_user_top_words.index)\n",
    "plot_values = list(top_20_user_top_words.values)\n",
    "sns.barplot(x=plot_index, y=plot_values, color=\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "600bc6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.pie(plot_values, labels=plot_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f868ae0b",
   "metadata": {},
   "source": [
    "### Check the words we let behind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1421a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "m="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e6a689",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "239.273px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
